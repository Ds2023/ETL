{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Creating a DAG**"
      ],
      "metadata": {
        "id": "klM3-UORpcgm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a dag that:\n",
        "\n",
        "\n",
        "*   extracts user information from /etc/passwd file\n",
        "*   transforms it, and loads it into a file\n",
        "\n"
      ],
      "metadata": {
        "id": "eeUv3SEKp8zY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LcV7VE5jpK4Z"
      },
      "outputs": [],
      "source": [
        "# Importing the necessary libraries\n",
        "\n",
        "# the dag object, to instantiate a dag\n",
        "from airflow import DAG\n",
        "from datetime import timedelta\n",
        "# operators in order to write tasks\n",
        "from airflow.operators.bash_operator import BashOperator\n",
        "# scheduling\n",
        "from airflow.utils.dates import days_ago\n",
        "\n",
        "# Defining dag arguments\n",
        "\n",
        "default_args = {\n",
        "    'owner': 'Ramesh Sannareddy',\n",
        "    'start_date':days_ago(0),\n",
        "    'email': ['ramesh@somemail.com'],\n",
        "    'email_on_failure': False,\n",
        "    'email_on_retry' : False,\n",
        "    'retries' : 1,\n",
        "    'retry_delay' : timedelta(minutes=5)\n",
        "}\n",
        "\n",
        "# defining the dag\n",
        "\n",
        "dag = DAG(\n",
        "    'my_first_dag',\n",
        "    default_args = default_args,\n",
        "    description=\"My first DAG\",\n",
        "    schedule_interval=timedelta(days=1)\n",
        ")\n",
        "\n",
        "# Task definition\n",
        "\n",
        "extract = BashOperator(\n",
        "    task_id='extract',\n",
        "    bash_command= 'cut -d\":\" -f1,3,6 /etc/passwd > /home/project/airflow/dags/extracted-data.txt',\n",
        "    dag=dag\n",
        ")\n",
        "\n",
        "transform_and_load = BashOperator(\n",
        "    task_id='transform',\n",
        "    bash_commands='tr \":\" \",\" < /home/project/airflow/dags/extracted-data.txt > /home/project/airflow/dags/transformed-data.csv',\n",
        "    dag=dag\n",
        ")\n",
        "\n",
        "# task pipeline\n",
        "\n",
        "extract >> transform_and_load"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Practice Exercise**"
      ],
      "metadata": {
        "id": "ZrFFnFLJquYi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem:\n",
        "Write a DAG named ETL_Server_Access_Log_Processing.\n",
        "\n",
        "Task 1: Create the imports block.\n",
        "\n",
        "Task 2: Create the DAG Arguments block. You can use the default settings\n",
        "\n",
        "Task 3: Create the DAG definition block. The DAG should run daily.\n",
        "\n",
        "Task 4: Create the download task.\n",
        "\n",
        "download task must download the server access log file which is available at the URL: https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DB0250EN-SkillsNetwork/labs/Apache%20Airflow/Build%20a%20DAG%20using%20Airflow/web-server-access-log.txt\n",
        "\n",
        "Task 5: Create the extract task.\n",
        "\n",
        "The server access log file contains these fields.\n",
        "\n",
        "a. timestamp - TIMESTAMP\n",
        "\n",
        "b. latitude - float\n",
        "\n",
        "c. longitude - float\n",
        "\n",
        "d. visitorid - char(37)\n",
        "\n",
        "e. accessed_from_mobile - boolean\n",
        "\n",
        "f. browser_code - int\n",
        "\n",
        "The extract task must extract the fields (timestamp) and (visitorid).\n",
        "\n",
        "Task 6: Create the transform task.\n",
        "\n",
        "The transform task must capitalize the visitorid.\n",
        "\n",
        "Task 7: Create the load task.\n",
        "\n",
        "The load task must compress the extracted and transformed data.\n",
        "\n",
        "Task 8: Create the task pipeline block."
      ],
      "metadata": {
        "id": "Echh16AyrM0Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing necessary libs\n",
        "from datetime import timedelta\n",
        "from airflow import DAG\n",
        "from airflow.operators.bash_operator import BashOperator\n",
        "from airflow.utils.dates import days_ago\n",
        "\n",
        "# dag arguments\n",
        "\n",
        "default_args = {\n",
        "    'owner': 'Ramesh Sannareddy',\n",
        "    'start_date': days_ago(0),\n",
        "    'email': ['ramesh@somemail.com'],\n",
        "    'email_on_failure': True,\n",
        "    'email_on_retry': True,\n",
        "    'retries': 1,\n",
        "    'retry_delay': timedelta(minutes=5),\n",
        "}\n",
        "\n",
        "# dag definition block\n",
        "\n",
        "dag = DAG(\n",
        "    'ETL_Server_Access_Log_Processing',\n",
        "    default_args = default_args,\n",
        "    description = 'second_dag',\n",
        "    schedule_interval=timedelta(days=1),\n",
        ")\n",
        "\n",
        "# creating the tasks\n",
        "\n",
        "download = BashOperator(\n",
        "    task_id = 'download',\n",
        "    bash_command = 'wget \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DB0250EN-SkillsNetwork/labs/Apache%20Airflow/Build%20a%20DAG%20using%20Airflow/web-server-access-log.txt\"',\n",
        "    dag=dag,\n",
        ")\n",
        "\n",
        "extract = BashOperator(\n",
        "    task_id='extract',\n",
        "    bash_command='cut -f1,4 -d\"#\" web-server-access-log.txt > /home/project/airflow/dags/extracted.txt',\n",
        "    dag=dag,\n",
        ")\n",
        "\n",
        "# transform task\n",
        "\n",
        "transform = BashOperator(\n",
        "    task_id='transform',\n",
        "    bash_command='tr \"[a-z]\" \"[A-Z]\" < /home/project/airflow/dags/extracted.txt > /home/project/airflow/dags/extracted.txt > /home/project/airflow/dags/capitalized.txt',\n",
        "    dag=dag,\n",
        ")\n",
        "\n",
        "# load data\n",
        "\n",
        "load = BashOperator(\n",
        "    task_id='load',\n",
        "    bash_command='zip log.zip capitalized.txt',\n",
        "    dag=dag,\n",
        ")\n",
        "\n",
        "# creating the pipeline\n",
        "download >> extract >> transform >> load\n",
        "\n",
        "# submitting the dag\n",
        "\n",
        "# in the terminal\n",
        "# we're copying the dag file to the dags folder in AIRFLOW_HOME directory\n",
        "\n",
        "cp  ETL_Server_Access_Log_Processing.py $AIRFLOW_HOME/dags\n",
        "\n",
        " # confirming the dag is submitted\n",
        " # list out all the dags in the airflow direcory\n",
        "\n",
        "airflow dags list\n",
        "\n"
      ],
      "metadata": {
        "id": "-COq5_BypXWo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}